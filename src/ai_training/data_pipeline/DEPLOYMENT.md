# ğŸš€ éƒ¨ç½²æŒ‡å— - åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ä½¿ç”¨æ¡†æ¶

## 1ï¸âƒ£ è®­ç»ƒé˜¶æ®µéƒ¨ç½²

### æ­¥éª¤1ï¼šå‡†å¤‡æ•°æ®
```python
from src.ai_training.data_pipeline import AutoDataPipeline

# åˆå§‹åŒ–ç®¡é“
pipeline = AutoDataPipeline(
    data_dir='./data/production/day_kline',
    period='day',
    random_seed=42,
    enable_logging=True
)

# è¿è¡Œç®¡é“
X_train, y_train, X_val, y_val, X_calibrate, y_calibrate, X_test, y_test, metadata = pipeline.run()
```

### æ­¥éª¤2ï¼šè®­ç»ƒæ¨¡å‹
```python
from autogluon.tabular import TabularPredictor

# ä½¿ç”¨DataFrameæ ¼å¼
train_df, val_df, _, _, metadata = pipeline.run_with_dataframe()

# è®­ç»ƒ
predictor = TabularPredictor(label='trend', path='./models/production_v1')
predictor.fit(
    train_data=train_df,
    tuning_data=val_df,
    time_limit=7200,
    presets='best_quality'
)
```

### æ­¥éª¤3ï¼šä¿å­˜å¤„ç†å·¥ä»¶
```python
import joblib
from pathlib import Path

model_dir = Path('./models/production_v1')

# ä¿å­˜å¤„ç†å™¨
pipeline.save_state(model_dir / 'data_pipeline.pkl')

# ä¿å­˜å…ƒæ•°æ®
import json
with open(model_dir / 'metadata.json', 'w') as f:
    json.dump(metadata, f, indent=2)

# ä¿å­˜ç‰¹å¾ååˆ—è¡¨
feature_names = pipeline.get_feature_names()
with open(model_dir / 'feature_names.json', 'w') as f:
    json.dump(feature_names, f)

print("âœ… æ‰€æœ‰å·¥ä»¶å·²ä¿å­˜")
```

---

## 2ï¸âƒ£ é¢„æµ‹é˜¶æ®µéƒ¨ç½²

### æ–¹æ¡ˆAï¼šä½¿ç”¨ä¿å­˜çš„å¤„ç†å™¨ï¼ˆæ¨èï¼‰

```python
import numpy as np
import joblib
import json
from pathlib import Path
from autogluon.tabular import TabularPredictor

class ProductionPredictorWrapper:
    """ç”Ÿäº§ç¯å¢ƒä¸­çš„é¢„æµ‹å™¨åŒ…è£…å™¨"""
    
    def __init__(self, model_dir: str):
        """
        åˆå§‹åŒ–é¢„æµ‹å™¨
        
        Args:
            model_dir: åŒ…å«å·²è®­ç»ƒæ¨¡å‹å’Œå¤„ç†å™¨çš„ç›®å½•
        """
        self.model_dir = Path(model_dir)
        
        # åŠ è½½æ¨¡å‹
        self.predictor = TabularPredictor.load(str(self.model_dir))
        
        # åŠ è½½å¤„ç†ç®¡é“
        self.pipeline = joblib.load(self.model_dir / 'data_pipeline.pkl')
        
        # åŠ è½½ç‰¹å¾å
        with open(self.model_dir / 'feature_names.json') as f:
            self.feature_names = json.load(f)
        
        # åŠ è½½å…ƒæ•°æ®ï¼ˆç”¨äºéªŒè¯ï¼‰
        with open(self.model_dir / 'metadata.json') as f:
            self.metadata = json.load(f)
    
    def preprocess(self, X_raw: np.ndarray) -> np.ndarray:
        """
        é¢„å¤„ç†åŸå§‹ç‰¹å¾
        
        Args:
            X_raw: åŸå§‹ç‰¹å¾ï¼Œå½¢çŠ¶ (N, 60, 51)
        
        Returns:
            é¢„å¤„ç†åçš„ç‰¹å¾ï¼Œå½¢çŠ¶ (N, 3060)
        """
        # æ‰å¹³åŒ–
        X_flat = X_raw.reshape(len(X_raw), -1)
        
        # è·å–å¤„ç†å™¨
        imputer = self.pipeline.get_imputer()
        scaler = self.pipeline.get_scaler()
        
        # åº”ç”¨å¤„ç†æ­¥éª¤
        X_clean = imputer.transform(X_flat)
        X_scaled = scaler.transform(X_clean)
        
        return X_scaled
    
    def predict(self, X_raw: np.ndarray) -> Dict[str, Any]:
        """
        é¢„æµ‹è¶‹åŠ¿
        
        Args:
            X_raw: åŸå§‹ç‰¹å¾ï¼Œå½¢çŠ¶ (N, 60, 51)
        
        Returns:
            é¢„æµ‹ç»“æœå­—å…¸
        """
        # é¢„å¤„ç†
        X_processed = self.preprocess(X_raw)
        
        # åˆ›å»ºDataFrame
        import pandas as pd
        X_df = pd.DataFrame(X_processed, columns=self.feature_names)
        
        # é¢„æµ‹
        predictions = self.predictor.predict(X_df)
        probabilities = self.predictor.predict_proba(X_df)
        
        # è½¬æ¢ç»“æœ
        trend_names = ['ä¸‹è·Œ', 'æ¨ªç›˜', 'ä¸Šæ¶¨']
        
        return {
            'predictions': predictions.tolist(),
            'probabilities': probabilities.values.tolist(),
            'trend_names': trend_names,
            'confidence': probabilities.max(axis=1).tolist()
        }
    
    def get_model_info(self) -> Dict:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        return {
            'trained_at': self.metadata['timestamp'],
            'period': self.metadata['period'],
            'total_samples': self.metadata['statistics']['total_samples'],
            'features_count': len(self.feature_names),
            'class_names': ['ä¸‹è·Œ', 'æ¨ªç›˜', 'ä¸Šæ¶¨']
        }


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == '__main__':
    # åˆå§‹åŒ–é¢„æµ‹å™¨
    predictor_wrapper = ProductionPredictorWrapper('./models/production_v1')
    
    # æ‰“å°æ¨¡å‹ä¿¡æ¯
    print("æ¨¡å‹ä¿¡æ¯:")
    for key, val in predictor_wrapper.get_model_info().items():
        print(f"  {key}: {val}")
    
    # é¢„æµ‹ï¼ˆå‡è®¾æœ‰æ–°æ•°æ®ï¼‰
    X_new = np.random.randn(10, 60, 51).astype(np.float32)  # ç¤ºä¾‹æ•°æ®
    results = predictor_wrapper.predict(X_new)
    
    print("\né¢„æµ‹ç»“æœ:")
    for i, (pred, prob, conf) in enumerate(zip(
        results['predictions'],
        results['probabilities'],
        results['confidence']
    )):
        trend = results['trend_names'][pred]
        print(f"  æ ·æœ¬ {i}: {trend} (ç½®ä¿¡åº¦: {conf:.2%})")
```

### æ–¹æ¡ˆBï¼šAPIæœåŠ¡éƒ¨ç½²

```python
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import numpy as np
from typing import List

app = FastAPI(title="è¶‹åŠ¿é¢„æµ‹ API")

# å…¨å±€åˆå§‹åŒ–ï¼ˆå¯åŠ¨æ—¶åŠ è½½ï¼‰
predictor_wrapper = None

@app.on_event("startup")
async def startup_event():
    """åº”ç”¨å¯åŠ¨æ—¶åŠ è½½æ¨¡å‹"""
    global predictor_wrapper
    predictor_wrapper = ProductionPredictorWrapper('./models/production_v1')
    print("âœ… æ¨¡å‹å·²åŠ è½½")

class PredictionRequest(BaseModel):
    """é¢„æµ‹è¯·æ±‚"""
    samples: List[List[List[float]]]  # (N, 60, 51)

class PredictionResponse(BaseModel):
    """é¢„æµ‹å“åº”"""
    predictions: List[int]
    probabilities: List[List[float]]
    confidence: List[float]
    trend_names: List[str]

@app.post("/predict", response_model=PredictionResponse)
async def predict(request: PredictionRequest):
    """
    è¶‹åŠ¿é¢„æµ‹ç«¯ç‚¹
    
    POST /predict
    {
        "samples": [[[...], [...], ...], ...]  # (N, 60, 51)
    }
    """
    try:
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        X = np.array(request.samples, dtype=np.float32)
        
        # éªŒè¯å½¢çŠ¶
        if X.shape[1:] != (60, 51):
            raise ValueError(f"ç‰¹å¾å½¢çŠ¶é”™è¯¯: {X.shape[1:]}, æœŸæœ› (60, 51)")
        
        # é¢„æµ‹
        results = predictor_wrapper.predict(X)
        
        return PredictionResponse(
            predictions=results['predictions'],
            probabilities=results['probabilities'],
            confidence=results['confidence'],
            trend_names=results['trend_names']
        )
    
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))

@app.get("/model/info")
async def get_model_info():
    """è·å–æ¨¡å‹ä¿¡æ¯"""
    return predictor_wrapper.get_model_info()

@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    return {"status": "healthy", "model": "loaded"}

# è¿è¡Œ: uvicorn app:app --reload
```

---

## 3ï¸âƒ£ å®¹å™¨åŒ–éƒ¨ç½²ï¼ˆDockerï¼‰

### Dockerfile

```dockerfile
FROM python:3.9-slim

WORKDIR /app

# å®‰è£…ä¾èµ–
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# å¤åˆ¶ä»£ç 
COPY src/ ./src/
COPY models/ ./models/
COPY app.py .

# æš´éœ²ç«¯å£
EXPOSE 8000

# å¯åŠ¨åº”ç”¨
CMD ["uvicorn", "app:app", "--host", "0.0.0.0", "--port", "8000"]
```

### requirements.txt

```
fastapi==0.104.0
uvicorn==0.24.0
numpy==1.24.0
pandas==2.0.0
scikit-learn==1.3.0
scipy==1.11.0
joblib==1.3.0
autogluon==0.8.0
loguru==0.7.0
```

### docker-compose.yml

```yaml
version: '3.8'

services:
  predictor:
    build: .
    ports:
      - "8000:8000"
    volumes:
      - ./models:/app/models:ro
      - ./logs:/app/logs
    environment:
      - MODEL_PATH=/app/models/production_v1
      - LOG_LEVEL=INFO
    restart: unless-stopped
```

### å¯åŠ¨å®¹å™¨

```bash
# æ„å»º
docker build -t trend-predictor:latest .

# è¿è¡Œ
docker run -p 8000:8000 \
  -v $(pwd)/models:/app/models:ro \
  trend-predictor:latest

# æˆ–ä½¿ç”¨docker-compose
docker-compose up -d
```

---

## 4ï¸âƒ£ ç›‘æ§å’Œæ—¥å¿—

### ç›‘æ§è„šæœ¬

```python
import logging
from datetime import datetime
import json

class ModelMonitor:
    """æ¨¡å‹ç›‘æ§"""
    
    def __init__(self, log_file: str = './logs/predictions.log'):
        self.log_file = log_file
        self.setup_logging()
    
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(self.log_file),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def log_prediction(self, X: np.ndarray, prediction: int, confidence: float):
        """è®°å½•é¢„æµ‹"""
        self.logger.info(f"Prediction: {prediction}, Confidence: {confidence:.2%}, Samples: {len(X)}")
    
    def log_error(self, error: str, details: dict = None):
        """è®°å½•é”™è¯¯"""
        msg = f"Error: {error}"
        if details:
            msg += f" | {json.dumps(details)}"
        self.logger.error(msg)
    
    def log_model_update(self, model_version: str, metrics: dict):
        """è®°å½•æ¨¡å‹æ›´æ–°"""
        msg = f"Model updated: v{model_version} | Metrics: {json.dumps(metrics)}"
        self.logger.info(msg)

# åœ¨APIä¸­ä½¿ç”¨
monitor = ModelMonitor()

@app.post("/predict")
async def predict(request: PredictionRequest):
    try:
        X = np.array(request.samples, dtype=np.float32)
        results = predictor_wrapper.predict(X)
        
        # è®°å½•æˆåŠŸé¢„æµ‹
        avg_conf = np.mean(results['confidence'])
        monitor.log_prediction(X, results['predictions'][0], avg_conf)
        
        return PredictionResponse(**results)
    except Exception as e:
        monitor.log_error(str(e), {'request_size': len(request.samples)})
        raise
```

---

## 5ï¸âƒ£ ç”Ÿäº§æ£€æŸ¥æ¸…å•

åœ¨éƒ¨ç½²å‰ï¼Œç¡®ä¿ï¼š

- [ ] æ¨¡å‹è®­ç»ƒå®Œæˆä¸”å‡†ç¡®ç‡è¾¾æ ‡
- [ ] æ‰€æœ‰å·¥ä»¶å·²ä¿å­˜ (æ¨¡å‹ã€å¤„ç†å™¨ã€å…ƒæ•°æ®)
- [ ] é¢„å¤„ç†é€»è¾‘ä¸è®­ç»ƒæ—¶ä¸€è‡´
- [ ] APIæ¥å£å·²æµ‹è¯•
- [ ] é”™è¯¯å¤„ç†å·²å®Œå–„
- [ ] æ—¥å¿—è®°å½•å·²å¯ç”¨
- [ ] ç›‘æ§å‘Šè­¦å·²è®¾ç½®
- [ ] æ–‡æ¡£å·²å®Œæˆ
- [ ] æ€§èƒ½æµ‹è¯•å·²é€šè¿‡
- [ ] è´Ÿè½½æµ‹è¯•å·²é€šè¿‡

---

## 6ï¸âƒ£ æ€§èƒ½ä¼˜åŒ–

### æ•°æ®é¢„å¤„ç†ä¼˜åŒ–

```python
# ç¼“å­˜å¤„ç†å™¨
class CachedPredictor(ProductionPredictorWrapper):
    def __init__(self, model_dir: str, cache_size: int = 1000):
        super().__init__(model_dir)
        self.cache_size = cache_size
        self.cache = {}
    
    def preprocess_batch(self, X_raw_list: List[np.ndarray]) -> np.ndarray:
        """æ‰¹é‡é¢„å¤„ç†"""
        # æ‰¹é‡æ“ä½œæ¯”å•ä¸ªå¿«
        X_batch = np.concatenate(X_raw_list)
        return self.preprocess(X_batch)
```

### å¹¶å‘å¤„ç†

```python
from concurrent.futures import ThreadPoolExecutor

class AsyncPredictor:
    def __init__(self, model_dir: str, workers: int = 4):
        self.wrapper = ProductionPredictorWrapper(model_dir)
        self.executor = ThreadPoolExecutor(max_workers=workers)
    
    async def predict_async(self, X_raw: np.ndarray):
        """å¼‚æ­¥é¢„æµ‹"""
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(
            self.executor,
            self.wrapper.predict,
            X_raw
        )
```

---

## 7ï¸âƒ£ æ•…éšœæ¢å¤

### è‡ªåŠ¨é‡å¯æœºåˆ¶

```python
import time
from tenacity import retry, stop_after_attempt, wait_exponential

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=2, max=10)
)
def load_predictor(model_dir: str):
    """å¸¦é‡è¯•çš„æ¨¡å‹åŠ è½½"""
    try:
        return ProductionPredictorWrapper(model_dir)
    except Exception as e:
        print(f"åŠ è½½å¤±è´¥ï¼Œé‡è¯•ä¸­: {e}")
        raise

# åˆå§‹åŒ–
try:
    predictor = load_predictor('./models/production_v1')
except Exception as e:
    print(f"æœ€ç»ˆå¤±è´¥: {e}")
    # å›é€€åˆ°å¤‡ç”¨æ¨¡å‹
    predictor = load_predictor('./models/backup_v0')
```

---

## ğŸ“Š éƒ¨ç½²æ£€æŸ¥è¡¨

```python
# éƒ¨ç½²å‰éªŒè¯è„šæœ¬
def verify_deployment(model_dir: str):
    """éªŒè¯éƒ¨ç½²å‡†å¤‡å°±ç»ª"""
    
    from pathlib import Path
    checks = []
    
    # 1. æ–‡ä»¶æ£€æŸ¥
    model_path = Path(model_dir)
    files_ok = all([
        (model_path / 'data_pipeline.pkl').exists(),
        (model_path / 'metadata.json').exists(),
        (model_path / 'feature_names.json').exists()
    ])
    checks.append(('æ–‡ä»¶æ£€æŸ¥', files_ok))
    
    # 2. æ¨¡å‹åŠ è½½æ£€æŸ¥
    try:
        predictor = ProductionPredictorWrapper(model_dir)
        checks.append(('æ¨¡å‹åŠ è½½', True))
    except Exception as e:
        checks.append(('æ¨¡å‹åŠ è½½', False))
    
    # 3. é¢„æµ‹æ£€æŸ¥
    try:
        X_test = np.random.randn(5, 60, 51).astype(np.float32)
        results = predictor.predict(X_test)
        assert len(results['predictions']) == 5
        checks.append(('é¢„æµ‹åŠŸèƒ½', True))
    except Exception as e:
        checks.append(('é¢„æµ‹åŠŸèƒ½', False))
    
    # 4. APIæ£€æŸ¥ï¼ˆå¦‚æœéƒ¨ç½²ä¸ºAPIï¼‰
    try:
        import requests
        resp = requests.get('http://localhost:8000/health', timeout=5)
        checks.append(('APIå¥åº·', resp.status_code == 200))
    except:
        checks.append(('APIå¥åº·', False))
    
    # æ‰“å°ç»“æœ
    print("\néƒ¨ç½²æ£€æŸ¥ç»“æœ:")
    print("=" * 40)
    for check_name, result in checks:
        status = "âœ…" if result else "âŒ"
        print(f"{status} {check_name}")
    print("=" * 40)
    
    return all(result for _, result in checks)

# è¿è¡ŒéªŒè¯
if __name__ == '__main__':
    is_ready = verify_deployment('./models/production_v1')
    if is_ready:
        print("\nâœ… éƒ¨ç½²å·²å‡†å¤‡å°±ç»ªï¼")
    else:
        print("\nâŒ éƒ¨ç½²æ£€æŸ¥å¤±è´¥ï¼Œè¯·ä¿®å¤ä¸Šè¿°é—®é¢˜ã€‚")
```

---

**ç°åœ¨ä½ å·²ç»å‡†å¤‡å¥½åœ¨ç”Ÿäº§ç¯å¢ƒä¸­éƒ¨ç½²ä½ çš„æ¨¡å‹äº†ï¼** ğŸš€
