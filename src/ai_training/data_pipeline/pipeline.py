"""
AutoDataPipeline - ä¸€ç«™å¼æ•°æ®å¤„ç†æ¡†æ¶

è‡ªåŠ¨å¤„ç†æ‰€æœ‰æ•°æ®åŠ è½½ã€éªŒè¯ã€æ¸…ç†ã€ç‰¹å¾å·¥ç¨‹ã€æ ‡å‡†åŒ–ã€åˆ†å‰²ç­‰æ­¥éª¤ã€‚
ç”¨æˆ·åªéœ€ä¸€è¡Œä»£ç ï¼Œæ¡†æ¶å¤„ç†æ‰€æœ‰ç»†èŠ‚ã€‚
"""

import numpy as np
import pandas as pd
import joblib
import json
from pathlib import Path
from datetime import datetime
from typing import Tuple, Dict, Optional, Any, List
from collections import defaultdict
import sys
import os

from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from scipy.spatial.distance import jensenshannon

sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..')))

from src.ai_training.kline_data_loader import StockImageAnalyzer
from src.ai_training.feature_extractor import FEATURE_NAMES, NUM_FEATURES, extract_features_sequence_from_kline_data

try:
    from loguru import logger
    HAS_LOGURU = True
except ImportError:
    class SimpleLogger:
        @staticmethod
        def info(msg): print(f"â„¹ï¸  {msg}")
        @staticmethod
        def warning(msg): print(f"âš ï¸  {msg}")
        @staticmethod
        def error(msg): print(f"âŒ {msg}")
        @staticmethod
        def success(msg): print(f"âœ… {msg}")
    logger = SimpleLogger()
    HAS_LOGURU = False


class PipelineStage:
    """ç®¡é“é˜¶æ®µæ‰§è¡ŒçŠ¶æ€"""
    def __init__(self, name: str):
        self.name = name
        self.start_time = None
        self.end_time = None
        self.status = 'pending'  # pending / running / success / failed
        self.error = None
        self.info = {}

    def start(self):
        import time
        self.start_time = time.time()
        self.status = 'running'

    def success(self, info: dict = None):
        import time
        self.end_time = time.time()
        self.status = 'success'
        if info:
            self.info.update(info)

    def fail(self, error: str):
        import time
        self.end_time = time.time()
        self.status = 'failed'
        self.error = error

    @property
    def duration(self) -> float:
        if self.start_time and self.end_time:
            return self.end_time - self.start_time
        return 0


class AutoDataPipeline:
    """
    è‡ªåŠ¨åŒ–æ•°æ®å¤„ç†ç®¡é“
    
    ä¸€è¡Œä»£ç å¤„ç†æ‰€æœ‰æ•°æ®é€»è¾‘ï¼Œæ— éœ€äººå·¥å¹²é¢„ã€‚
    """

    SEQUENCE_LENGTH = 60
    FEATURES_PER_STEP = 51
    TOTAL_FEATURES = SEQUENCE_LENGTH * FEATURES_PER_STEP

    SMALL_DATA_THRESHOLD = 500
    MIN_TOTAL_SAMPLES = 60
    MIN_SAMPLES_PER_CLASS = 20
    IMBALANCE_RATIO_THRESHOLD = 2.0

    JS_DIVERGENCE_EXCELLENT = 0.02
    JS_DIVERGENCE_GOOD = 0.05
    JS_DIVERGENCE_ACCEPTABLE = 0.10

    def __init__(
        self,
        data_dir: str,
        period: str = 'auto',
        enable_logging: bool = True,
        random_seed: int = 42,
        min_samples_per_class: int = 20,
        imbalance_threshold: float = 2.0,
        config: dict = None
    ):
        """
        åˆå§‹åŒ–ç®¡é“
        
        Args:
            data_dir: æ•°æ®ç›®å½•è·¯å¾„
            period: Kçº¿å‘¨æœŸï¼ˆauto/day/5min/15minç­‰ï¼‰
            enable_logging: æ˜¯å¦å¯ç”¨æ—¥å¿—
            random_seed: éšæœºç§å­
            min_samples_per_class: æ¯ç±»æœ€å°‘æ ·æœ¬æ•°
            imbalance_threshold: ç±»åˆ«ä¸å¹³è¡¡è­¦å‘Šé˜ˆå€¼
            config: è‡ªå®šä¹‰é…ç½®å­—å…¸
        """
        self.data_dir = Path(data_dir)
        self.period = self._detect_period(period)
        self.random_seed = random_seed
        self.min_samples_per_class = min_samples_per_class
        self.imbalance_threshold = imbalance_threshold
        self.config = config or {}

        # æ•°æ®å®¹å™¨
        self.X = None
        self.y = None
        self.actual_returns = None

        self.X_train = self.X_val = self.X_calibrate = self.X_test = None
        self.y_train = self.y_val = self.y_calibrate = self.y_test = None
        self.returns_train = self.returns_val = self.returns_calibrate = self.returns_test = None

        # å¤„ç†å™¨
        self.imputer = None
        self.scaler = None

        # å…ƒæ•°æ®
        self.metadata = {
            'timestamp': datetime.now().isoformat(),
            'version': '1.0.0',
            'period': self.period,
            'processing_log': [],
            'warnings': [],
            'statistics': {},
            'stages': {}
        }

        # é˜¶æ®µè·Ÿè¸ª
        self.stages = {}

    def _detect_period(self, period: str) -> str:
        """è‡ªåŠ¨æ£€æµ‹Kçº¿å‘¨æœŸ"""
        if period != 'auto':
            return period

        period_map = {
            '5min': '5min',
            '15min': '15min',
            '30min': '30min',
            '1hour': '1hour',
            'week': 'week'
        }

        for key, val in period_map.items():
            if key in str(self.data_dir):
                return val

        return 'day'

    def _log(self, msg: str, level: str = 'info'):
        """è®°å½•æ¶ˆæ¯"""
        if level == 'info':
            logger.info(msg)
        elif level == 'warning':
            logger.warning(msg)
            self.metadata['warnings'].append(msg)
        elif level == 'error':
            logger.error(msg)
        elif level == 'success':
            logger.success(msg)

        self.metadata['processing_log'].append(f"[{level.upper()}] {msg}")

    def _create_stage(self, name: str) -> PipelineStage:
        """åˆ›å»ºç®¡é“é˜¶æ®µ"""
        stage = PipelineStage(name)
        self.stages[name] = stage
        return stage

    def load_data(self) -> 'AutoDataPipeline':
        """åŠ è½½æ•°æ®ï¼ˆå¯é“¾å¼è°ƒç”¨ï¼‰"""
        stage = self._create_stage('load')
        stage.start()

        try:
            self._log("ğŸ“‚ åŠ è½½è®­ç»ƒæ•°æ®...")

            analyzer = StockImageAnalyzer(enable_database=True)
            features_list = []
            labels_list = []
            returns_list = []

            trend_dirs = {'down_trend': 0, 'sideways': 1, 'up_trend': 2}

            for trend_name, label in trend_dirs.items():
                trend_path = self.data_dir / trend_name
                json_file = trend_path / 'data.json'

                if not json_file.exists():
                    self._log(f"{json_file} ä¸å­˜åœ¨ï¼Œè·³è¿‡", 'warning')
                    continue

                self._log(f"åŠ è½½ {trend_name}...")

                try:
                    results = analyzer.get_training_data_from_json(str(json_file))
                    if not results:
                        self._log(f"{trend_name} æ— æœ‰æ•ˆæ•°æ®", 'warning')
                        continue

                    count = 0
                    for item in results:
                        try:
                            features = extract_features_sequence_from_kline_data(
                                item['kline_data'],
                                item['period'],
                                item.get('market_index_klines'),
                                item.get('stock_code')
                            )

                            if features is not None:
                                features_list.append(features)
                                labels_list.append(label)
                                returns_list.append(item.get('actual_return', None))
                                count += 1
                        except Exception:
                            continue

                    self._log(f"{trend_name}: {count} æ¡æˆåŠŸ", 'success')

                except Exception as e:
                    self._log(f"åŠ è½½ {trend_name} å¤±è´¥: {e}", 'error')
                    continue

            if not features_list:
                raise ValueError("æ²¡æœ‰åŠ è½½åˆ°æœ‰æ•ˆæ•°æ®")

            self.X = np.array(features_list, dtype=np.float32)
            self.y = np.array(labels_list, dtype=np.int32)
            self.actual_returns = np.array(returns_list, dtype=np.float32)

            # æ‰“ä¹±æ•°æ®
            rng = np.random.RandomState(self.random_seed)
            shuffle_idx = rng.permutation(len(self.X))
            self.X = self.X[shuffle_idx]
            self.y = self.y[shuffle_idx]
            self.actual_returns = self.actual_returns[shuffle_idx]

            stage.success({
                'total_samples': len(self.X),
                'shape': str(self.X.shape),
                'down_trend': int(np.sum(self.y == 0)),
                'sideways': int(np.sum(self.y == 1)),
                'up_trend': int(np.sum(self.y == 2))
            })

            self._log(f"âœ… åŠ è½½å®Œæˆ: {len(self.X)} æ¡æ ·æœ¬", 'success')

        except Exception as e:
            stage.fail(str(e))
            raise

        return self

    def validate_data(self) -> 'AutoDataPipeline':
        """éªŒè¯æ•°æ®"""
        stage = self._create_stage('validate')
        stage.start()

        try:
            self._log("ğŸ“‹ æ•°æ®éªŒè¯...")

            # ç»´åº¦éªŒè¯
            assert self.X.ndim == 3, f"ç‰¹å¾ç»´åº¦é”™è¯¯: {self.X.ndim}Dï¼ŒæœŸæœ›3D"
            assert self.X.shape[1] == self.SEQUENCE_LENGTH, f"åºåˆ—é•¿åº¦é”™è¯¯"
            assert self.X.shape[2] == NUM_FEATURES, f"ç‰¹å¾æ•°é”™è¯¯"
            assert self.y.ndim == 1, f"æ ‡ç­¾ç»´åº¦é”™è¯¯"
            assert len(self.X) == len(self.y), f"æ ·æœ¬æ•°ä¸åŒ¹é…"

            # ç±»åˆ«éªŒè¯
            unique_classes = np.unique(self.y)
            assert np.array_equal(unique_classes, [0, 1, 2]), f"ç±»åˆ«å€¼é”™è¯¯: {unique_classes}"

            # NaNæ£€æŸ¥
            nan_count = np.isnan(self.X).sum()
            if nan_count > 0:
                self._log(f"å‘ç° {nan_count} ä¸ªNaNå€¼", 'warning')

            # æ ·æœ¬æ•°æ£€æŸ¥
            if len(self.X) < self.MIN_TOTAL_SAMPLES:
                self._log(f"æ ·æœ¬æ•°è¾ƒå°‘ ({len(self.X)} æ¡)", 'warning')

            # ç±»åˆ«ä¸å¹³è¡¡æ£€æŸ¥
            class_counts = np.bincount(self.y)
            imbalance_ratio = np.max(class_counts) / np.min(class_counts)
            if imbalance_ratio > self.IMBALANCE_RATIO_THRESHOLD:
                self._log(f"æ•°æ®ä¸å¹³è¡¡æ¯”ä¾‹: {imbalance_ratio:.2f}:1", 'warning')

            stage.success({
                'dimensions_ok': True,
                'nan_count': int(nan_count),
                'imbalance_ratio': float(imbalance_ratio)
            })

            self._log("âœ… éªŒè¯é€šè¿‡", 'success')

        except Exception as e:
            stage.fail(str(e))
            raise

        return self

    def clean_data(self) -> 'AutoDataPipeline':
        """æ¸…ç†æ•°æ®ï¼ˆå¤„ç†NaN/Infï¼‰"""
        stage = self._create_stage('clean')
        stage.start()

        try:
            self._log("ğŸ§¹ æ•°æ®æ¸…ç†...")

            # æ£€æŸ¥Inf
            inf_count = np.isinf(self.X).sum()
            if inf_count > 0:
                self._log(f"å‘ç° {inf_count} ä¸ªInfå€¼ï¼Œè½¬ä¸ºNaN", 'warning')
                self.X = self.X.copy()
                self.X[np.isinf(self.X)] = np.nan

            # æ£€æŸ¥NaN
            nan_count = np.isnan(self.X).sum()
            if nan_count > 0:
                self._log(f"ä½¿ç”¨SimpleImputerå¡«å…… {nan_count} ä¸ªNaNå€¼", 'info')
                X_flat = self.X.reshape(len(self.X), -1)
                self.imputer = SimpleImputer(strategy='mean')
                X_flat = self.imputer.fit_transform(X_flat)
                self.X = X_flat.reshape(self.X.shape)

            if self.imputer is None:
                # å³ä½¿æ²¡æœ‰NaNï¼Œä¹Ÿåˆ›å»ºimputerï¼ˆä¿è¯æ¥å£ä¸€è‡´ï¼‰
                X_flat = self.X.reshape(len(self.X), -1)
                self.imputer = SimpleImputer(strategy='mean')
                X_flat = self.imputer.fit_transform(X_flat)
                self.X = X_flat.reshape(self.X.shape)

            stage.success({
                'nan_count': int(nan_count),
                'inf_count': int(inf_count),
                'shape': str(self.X.shape)
            })

            self._log("âœ… æ¸…ç†å®Œæˆ", 'success')

        except Exception as e:
            stage.fail(str(e))
            raise

        return self

    def engineer_features(self) -> 'AutoDataPipeline':
        """ç‰¹å¾å·¥ç¨‹ï¼ˆç”Ÿæˆè¯­ä¹‰åŒ–ç‰¹å¾åï¼‰"""
        stage = self._create_stage('engineer')
        stage.start()

        try:
            self._log("ğŸ”§ ç‰¹å¾å·¥ç¨‹...")

            # éªŒè¯ç‰¹å¾æ•°
            if len(FEATURE_NAMES) != self.FEATURES_PER_STEP:
                raise ValueError(f"ç‰¹å¾åè®¡æ•°é”™è¯¯: {len(FEATURE_NAMES)} != {self.FEATURES_PER_STEP}")

            # ç”Ÿæˆç‰¹å¾å
            self.feature_names = []
            for k_idx in range(self.SEQUENCE_LENGTH):
                for feat_name in FEATURE_NAMES:
                    self.feature_names.append(f"K{k_idx}_{feat_name}")

            assert len(self.feature_names) == self.TOTAL_FEATURES

            stage.success({
                'feature_count': len(self.feature_names),
                'sample_features': self.feature_names[:5]
            })

            self._log(f"âœ… ç”Ÿæˆ {len(self.feature_names)} ä¸ªè¯­ä¹‰åŒ–ç‰¹å¾å", 'success')

        except Exception as e:
            stage.fail(str(e))
            raise

        return self

    def standardize_data(self) -> 'AutoDataPipeline':
        """æ ‡å‡†åŒ–æ•°æ®"""
        stage = self._create_stage('standardize')
        stage.start()

        try:
            self._log("ğŸ“Š æ ‡å‡†åŒ–æ•°æ®...")

            # æ‰å¹³åŒ–
            X_flat = self.X.reshape(len(self.X), -1)

            # æ ‡å‡†åŒ–
            self.scaler = StandardScaler()
            X_flat = self.scaler.fit_transform(X_flat)

            # éªŒè¯
            if np.any(np.isnan(X_flat)) or np.any(np.isinf(X_flat)):
                raise ValueError("æ ‡å‡†åŒ–ååŒ…å«NaN/Inf")

            # é‡å¡‘
            self.X = X_flat.reshape(len(self.X), self.SEQUENCE_LENGTH, NUM_FEATURES)

            stage.success({
                'mean': float(self.X.mean()),
                'std': float(self.X.std()),
                'shape': str(self.X.shape)
            })

            self._log("âœ… æ ‡å‡†åŒ–å®Œæˆ", 'success')

        except Exception as e:
            stage.fail(str(e))
            raise

        return self

    def split_data(self) -> 'AutoDataPipeline':
        """åˆ†å‰²æ•°æ®"""
        stage = self._create_stage('split')
        stage.start()

        try:
            self._log("âœ‚ï¸ æ•°æ®åˆ†å‰²...")

            n_samples = len(self.X)

            if n_samples < self.SMALL_DATA_THRESHOLD:
                self._log(f"å°æ•°æ®æ¨¡å¼ (N={n_samples}): 3-Way Split", 'info')
                train_ratio, val_ratio, test_ratio = 0.70, 0.15, 0.15

                train_size = int(n_samples * train_ratio)
                val_size = int(n_samples * val_ratio)

                self.X_train = self.X[:train_size]
                self.X_val = self.X[train_size:train_size + val_size]
                self.X_test = self.X[train_size + val_size:]

                self.y_train = self.y[:train_size]
                self.y_val = self.y[train_size:train_size + val_size]
                self.y_test = self.y[train_size + val_size:]

                self.X_calibrate = self.X_val.copy()
                self.y_calibrate = self.y_val.copy()

                if self.actual_returns is not None:
                    self.returns_train = self.actual_returns[:train_size]
                    self.returns_val = self.actual_returns[train_size:train_size + val_size]
                    self.returns_test = self.actual_returns[train_size + val_size:]
                    self.returns_calibrate = self.returns_val.copy()

            else:
                self._log(f"å¤§æ•°æ®æ¨¡å¼ (N={n_samples}): 4-Way Split", 'info')
                train_ratio, val_ratio, calibrate_ratio, test_ratio = 0.50, 0.20, 0.20, 0.10

                train_size = int(n_samples * train_ratio)
                val_size = int(n_samples * val_ratio)
                calibrate_size = int(n_samples * calibrate_ratio)

                self.X_train = self.X[:train_size]
                self.X_val = self.X[train_size:train_size + val_size]
                self.X_calibrate = self.X[train_size + val_size:train_size + val_size + calibrate_size]
                self.X_test = self.X[train_size + val_size + calibrate_size:]

                self.y_train = self.y[:train_size]
                self.y_val = self.y[train_size:train_size + val_size]
                self.y_calibrate = self.y[train_size + val_size:train_size + val_size + calibrate_size]
                self.y_test = self.y[train_size + val_size + calibrate_size:]

                if self.actual_returns is not None:
                    self.returns_train = self.actual_returns[:train_size]
                    self.returns_val = self.actual_returns[train_size:train_size + val_size]
                    self.returns_calibrate = self.actual_returns[train_size + val_size:train_size + val_size + calibrate_size]
                    self.returns_test = self.actual_returns[train_size + val_size + calibrate_size:]

            stage.success({
                'train_size': len(self.X_train),
                'val_size': len(self.X_val),
                'calibrate_size': len(self.X_calibrate),
                'test_size': len(self.X_test)
            })

            self._log("âœ… åˆ†å‰²å®Œæˆ", 'success')

        except Exception as e:
            stage.fail(str(e))
            raise

        return self

    def validate_consistency(self) -> 'AutoDataPipeline':
        """éªŒè¯åˆ†å¸ƒä¸€è‡´æ€§"""
        stage = self._create_stage('consistency')
        stage.start()

        try:
            self._log("ğŸ“ åˆ†å¸ƒä¸€è‡´æ€§éªŒè¯...")

            train_dist = np.bincount(self.y_train, minlength=3) / len(self.y_train)
            calibrate_dist = np.bincount(self.y_calibrate, minlength=3) / len(self.y_calibrate)

            js_divergence = jensenshannon(train_dist, calibrate_dist)

            if js_divergence < self.JS_DIVERGENCE_EXCELLENT:
                status = 'ä¼˜ç§€'
            elif js_divergence < self.JS_DIVERGENCE_GOOD:
                status = 'è‰¯å¥½'
            elif js_divergence < self.JS_DIVERGENCE_ACCEPTABLE:
                status = 'å¯æ¥å—'
            else:
                status = 'è­¦å‘Š'

            self._log(f"Jensen-Shannonæ•£åº¦: {js_divergence:.4f} ({status})", 'info')

            stage.success({
                'js_divergence': float(js_divergence),
                'status': status,
                'train_distribution': train_dist.tolist(),
                'calibrate_distribution': calibrate_dist.tolist()
            })

            self._log("âœ… ä¸€è‡´æ€§éªŒè¯å®Œæˆ", 'success')

        except Exception as e:
            stage.fail(str(e))
            raise

        return self

    def run(self) -> Tuple:
        """
        ä¸€é”®è¿è¡Œå®Œæ•´ç®¡é“
        
        Returns:
            (X_train, y_train, X_val, y_val, X_calibrate, y_calibrate, X_test, y_test, metadata)
        """
        self._log("="*70)
        self._log("ğŸš€ AutoDataPipeline å¼€å§‹è¿è¡Œ")
        self._log("="*70)

        try:
            self.load_data()
            self.validate_data()
            self.clean_data()
            self.engineer_features()
            self.standardize_data()
            self.split_data()
            self.validate_consistency()

            # æ„å»ºå…ƒæ•°æ®
            self._build_metadata()

            self._log("="*70)
            self._log("âœ… ç®¡é“è¿è¡Œå®Œæˆ")
            self._log("="*70)

            return (
                self.X_train, self.y_train,
                self.X_val, self.y_val,
                self.X_calibrate, self.y_calibrate,
                self.X_test, self.y_test,
                self.metadata
            )

        except Exception as e:
            self._log(f"âŒ ç®¡é“è¿è¡Œå¤±è´¥: {e}", 'error')
            raise

    def run_with_dataframe(self) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame, Dict]:
        """
        è¿è¡Œç®¡é“å¹¶è¿”å›DataFrameæ ¼å¼ï¼ˆç”¨äºAutoGluonï¼‰
        
        Returns:
            (train_df, val_df, calibrate_df, test_df, metadata)
        """
        X_train, y_train, X_val, y_val, X_calibrate, y_calibrate, X_test, y_test, metadata = self.run()

        # æ‰å¹³åŒ–
        X_train_flat = X_train.reshape(len(X_train), -1)
        X_val_flat = X_val.reshape(len(X_val), -1)
        X_calibrate_flat = X_calibrate.reshape(len(X_calibrate), -1)
        X_test_flat = X_test.reshape(len(X_test), -1)

        # åˆ›å»ºDataFrame
        train_df = pd.DataFrame(X_train_flat, columns=self.feature_names)
        train_df['trend'] = y_train

        val_df = pd.DataFrame(X_val_flat, columns=self.feature_names)
        val_df['trend'] = y_val

        calibrate_df = pd.DataFrame(X_calibrate_flat, columns=self.feature_names)
        calibrate_df['trend'] = y_calibrate

        test_df = pd.DataFrame(X_test_flat, columns=self.feature_names)
        test_df['trend'] = y_test

        return train_df, val_df, calibrate_df, test_df, metadata

    def _build_metadata(self):
        """æ„å»ºå®Œæ•´å…ƒæ•°æ®"""
        self.metadata['data_split'] = {
            'train': len(self.X_train),
            'val': len(self.X_val),
            'calibrate': len(self.X_calibrate),
            'test': len(self.X_test)
        }

        self.metadata['class_distribution'] = {
            'down_trend': int(np.sum(self.y == 0)),
            'sideways': int(np.sum(self.y == 1)),
            'up_trend': int(np.sum(self.y == 2))
        }

        self.metadata['statistics'] = {
            'total_samples': len(self.X),
            'total_features': len(self.feature_names),
            'feature_mean': float(self.X.mean()),
            'feature_std': float(self.X.std())
        }

        self.metadata['stages'] = {
            name: {
                'status': stage.status,
                'duration': stage.duration,
                'info': stage.info,
                'error': stage.error
            }
            for name, stage in self.stages.items()
        }

    def get_metadata(self) -> Dict:
        """è·å–å…ƒæ•°æ®"""
        return self.metadata

    def get_stage_info(self, stage_name: str) -> Dict:
        """è·å–ç‰¹å®šé˜¶æ®µçš„ä¿¡æ¯"""
        if stage_name not in self.stages:
            return {}
        stage = self.stages[stage_name]
        return {
            'status': stage.status,
            'duration': f"{stage.duration:.2f}s",
            'info': stage.info
        }

    def get_scaler(self) -> StandardScaler:
        """è·å–æ ‡å‡†åŒ–å™¨ï¼ˆç”¨äºé¢„æµ‹æ—¶å¤ç°ï¼‰"""
        if self.scaler is None:
            raise RuntimeError("Scaler not initialized. Run pipeline first.")
        return self.scaler

    def get_imputer(self) -> SimpleImputer:
        """è·å–å¡«å……å™¨ï¼ˆç”¨äºé¢„æµ‹æ—¶å¤ç°ï¼‰"""
        if self.imputer is None:
            raise RuntimeError("Imputer not initialized. Run pipeline first.")
        return self.imputer

    def get_feature_names(self) -> List[str]:
        """è·å–ç‰¹å¾ååˆ—è¡¨"""
        if not hasattr(self, 'feature_names'):
            raise RuntimeError("Feature names not generated. Run pipeline first.")
        return self.feature_names

    def print_full_log(self):
        """æ‰“å°å®Œæ•´æ—¥å¿—"""
        print("\n" + "="*70)
        print("ğŸ“‹ å®Œæ•´å¤„ç†æ—¥å¿—")
        print("="*70)
        for log in self.metadata['processing_log']:
            print(log)

    def get_processing_report(self) -> str:
        """è·å–å¤„ç†æŠ¥å‘Š"""
        report = []
        report.append("="*70)
        report.append("âœ… æ•°æ®å¤„ç†å®ŒæˆæŠ¥å‘Š")
        report.append("="*70)

        report.append("\nğŸ“Š æ•°æ®ç»Ÿè®¡")
        for key, val in self.metadata['statistics'].items():
            if isinstance(val, float):
                report.append(f"  {key}: {val:.6f}")
            else:
                report.append(f"  {key}: {val}")

        report.append("\nğŸ“ˆ ç±»åˆ«åˆ†å¸ƒ")
        for trend, count in self.metadata['class_distribution'].items():
            pct = count / self.metadata['statistics']['total_samples'] * 100
            report.append(f"  {trend}: {count} ({pct:.1f}%)")

        report.append("\nâ±ï¸ å¤„ç†æ—¶é—´")
        total_duration = sum(s['duration'] for s in self.metadata['stages'].values() if isinstance(s.get('duration'), (int, float)))
        for stage_name, stage_info in self.metadata['stages'].items():
            if isinstance(stage_info.get('duration'), (int, float)):
                report.append(f"  {stage_name}: {stage_info['duration']:.2f}s")
        report.append(f"  æ€»è€—æ—¶: {total_duration:.2f}s")

        if self.metadata['warnings']:
            report.append("\nâš ï¸ è­¦å‘Š")
            for warn in self.metadata['warnings']:
                report.append(f"  - {warn}")
        else:
            report.append("\nâœ… æ— è­¦å‘Š")

        return "\n".join(report)

    def save_state(self, path: str):
        """ä¿å­˜å¤„ç†çŠ¶æ€"""
        state = {
            'scaler': self.scaler,
            'imputer': self.imputer,
            'feature_names': self.feature_names if hasattr(self, 'feature_names') else None,
            'metadata': self.metadata,
            'config': self.config
        }
        joblib.dump(state, path)
        self._log(f"çŠ¶æ€å·²ä¿å­˜åˆ° {path}", 'success')

    def load_state(self, path: str):
        """åŠ è½½å¤„ç†çŠ¶æ€"""
        state = joblib.load(path)
        self.scaler = state['scaler']
        self.imputer = state['imputer']
        self.feature_names = state['feature_names']
        self.metadata = state['metadata']
        self.config = state['config']
        self._log(f"çŠ¶æ€å·²ä» {path} åŠ è½½", 'success')
